[ { "title": "Making an Automated Google BigQuery Data Quality Monitor with Slack Alerts", "url": "/posts/big-query-data-quality-monitor-slack/", "categories": "Business Intelligence", "tags": "CRM Analytics, Business Intelligence", "date": "2022-07-24 00:00:00 -0500", "snippet": "The PremiseBusiness Intelligence is all about using data to help drive decision making. Inherent in that definition is the assumption that the data is correct. In the real world, however, data can occasionally be wrong. Maybe your ETL script has a small bug. Maybe an upstream data source messed up. Maybe a DAG didn’t finish fully.There are tons of possible causes, all with the same end result: your data is incomplete and/or incorrect. In my mind, the worst possible outcome in this situation is that you unknowingly surface that incorrect data to decision makers. This might incorrectly influence the decisions and frankly defeat the whole purpose of the business intelligence organization.So if your data is wrong, you need to know. And you need to know immediately so that you can take corrective action. There are a lot of different approaches to doing this, and I’m going to break down how I accomplished it for my team.Solution OverviewOur team uses BigQuery to stage our data before ingesting it into CRM Analytics for end user consumption. Right now we do all our orchestration through cron scheduling of our Python ETL scripts, but we have plans to move to AirFlow for orchestration in the near future. Building out a cloud-native approach for the data quality testing suite that works with both our current and future orchestration was my top priority.In a nutshell, the solution works like this: Tests are defined as BigQuery scheduled queries, with one scheduled query per table. Multiple tests are defined in each script, and a test failure will raise a SQL ERROR() The scheduled query is either scheduled to run at certain times of the day or can be invoked on demand via Python Scheduled query errors are published to a Pub/Sub topic A cloud function subscribes to the Pub/Sub topic, so when it sees an error it then uses an Incoming Webhook to send a message to Slack A Slack app receives the Incoming Webhook and sends the alert to your team’s Slack channel, alerting you in real-time to the data quality error.Tada!Why I like this approachThere are a few reasons why I like this approach that I think are worth sharing. Can be implemented entirely within the Free Tier of Google Cloud Platform, so there’s no additional cost. Because it lives entirely within GCP there’s no additional software or platform required. The regression tests are simple and easy-to-understand SQL, so anyone on your team can add to the test suite.Setup GuideOne of the beauties of this solution is that it’s relatively easy to set up: from start to finish, you can have your first test suite running in an hour or two. (Of course, if you’re working at a larger enterprise like mine, dealing with the Slack and GCP IAM permissions for some of these steps will add days to the process…)1. Create Pub/Sub TopicThe first step is to create your pub/sub topic. Give it a relevant topic id such as “data-alerts”.The “Add a default subscription” checkbox is optional. You don’t need a default subscription for this process - the cloud function will create its own subscription - but it can be nice to have if you want to manually pull your failure event messages for troubleshooting purposes.Leave the schema and message retention checkboxes unchecked. Checking the schema option will likely cause this process to fail, and there’s no need to retain the pub/sub messages because the failure events are already stored in StackDriver/Logs Explorer.2. Create Destination TableNow switch over to BigQuery. We need to create a new table that will store the results of successful test runs. Our scheduled query will use SELECT statements, and we have to specify a destination table to store the results of those SELECT statements or we’ll get an error. The upside to this is that this table can serve as a log of all your successful test runs, which is cool.Give the table a relevant name such as data_quality_test_results, and create two columns: test_info (STRING) etl_timestamp (TIMESTAMP)3. Create Scheduled QueryNow that we have a Pub/Sub topic to receive test failures and a destination table to store successful test runs, we’re ready to create our first test suite.Within BigQuery, open a new editor window and construct a test similar to the following. -- This query will run a series of tests to ensure data quality within the new_table table -- Each query needs a test_info string formatted as \"&lt;TEST_NAME&gt;::&lt;TABLE_NAME&gt;::\" -- Ensure table isn't emptySELECTIF (row_count = 0, ERROR(test_info), test_info) AS test_info, CURRENT_TIMESTAMP() AS etl_timestampFROM ( SELECT COUNT(*) AS row_count, \"Table is Empty::new_table::\" AS test_info FROM `my-project-name.test.new_table` )UNION ALL -- Ensure that first name and last name are either both provided or both nullSELECTIF (row_count &gt; 0, ERROR(test_info), test_info) AS test_info, CURRENT_TIMESTAMP() AS etl_timestampFROM ( SELECT COUNT(*) AS row_count, \"Incomplete name fields::new_table::\" AS test_info FROM `my-project-name.test.new_table` WHERE (NOT(first_name IS NULL) AND last_name IS NULL) OR (first_name IS NULL AND NOT(last_name IS NULL)) )Now obviously this is a simple example, where I perform two tests: A check to ensure that the table has a row count greater than 0. A check to ensure that all the records in this table either have no first_name/last_name or have both a first_name &amp; last_name.You’ll want to tweak the tests to suit your needs. The possibilities are endless, but you might: Check to ensure profit &gt; 0 each day Check to ensure that join keys such as employee_id are coming through in the proper format (int versus float, etc) Check to ensure the product_name field doesn’t contain erroneous or unexpected values. Etc.The most important thing to remember is that the ERROR raised by each test contains a string with the test name and table name, each followed by two colons. We’ll need that string in the cloud function, so you can’t omit it.Scheduling the queryNow that the query has been created, you need to schedule it. Press the Schedule button at the top of the editor window and create a new scheduled query.First, give your query a name such as “Data Quality Test: table_name”. Following a set nomenclature like this will help you easily find the test suite you need if you have a lot of scheduled queries.Next, choose how often to run the test. If your tables are refreshed every day at the same time, you can easily schedule the query to run a minute or two later.If your tables aren’t refreshed on a predictable cadence, you can also choose the “On-demand” option from the Repeats dropdown. At that point, you can either use the web ui or the Data Transfer API to initiate your tests when needed.Under the Destination for query results section, select the destination table you created in Step 2 above. Make sure the “Append to table” radio button is selected so that the table continues to aggregate all successful runs.Finally, under the notification options section, enter the pub/sub topic you created in Step 1 above.4. Create Slack AppIt’s time to create our Slack app.Go to the Slack Apps page and click the Create New App button. Choose the ‘From scratch’ option, then give your app a killer name and choose the Workspace where you want this app to live.Once the app is created, go to the Incoming Webhooks page and activate the feature. Scroll to the bottom of the page and click the “Add New Webhook to Workspace” button. Now select which channel your App should post alerts to and click ‘Allow’.You’ll be returned to the Incoming Webhooks page where you can now see a URL that starts with https://hooks.slack.com. You’ll need this URL for the next step.That’s it! Before moving on to the next step, you might want to give your app a sweet icon and meaningful description under the Basic Information section.5. Create Cloud FunctionWe’re almost done! We now have automated data quality tests running, which publish to a pub/sub topic when bad data is found. We also have a Slack app that will alert us to those test failures. We just need a relay to tie them together.Head back to your Google Cloud console and open up Cloud Functions. Create a new function.Set environment to 1st gen and give your function a name, such as “data-quality-slack-notification”.Now under the Trigger section, select “Cloud Pub/Sub” as the trigger type and select the pub/sub topic you created earlier. Hit Save, then Next.Cloud Functions support a number of runtimes, but I made this in Node.js so you’ll want to select Node.js 16 as the runtime.Enter the following code into the package.json file:{ \"name\": \"data-quality-slack-notification\", \"version\": \"0.0.1\", \"dependencies\": { \"@google-cloud/pubsub\": \"^0.18.0\", \"@slack/webhook\": \"^6.1.0\" }}Now switch back to the index.js file, edit the entry point to “subscribe” and enter the following code:const {IncomingWebhook} = require('@slack/webhook');const SLACK_WEBHOOK_URL = \"&lt;YOUR WEBHOOK URL FROM STEP3&gt;\";const webhook = new IncomingWebhook(SLACK_WEBHOOK_URL);// subscribe is the main function called by Cloud Functionsmodule.exports.subscribe = (event) =&gt; { const eventData = getMessageData(event.data); //Only send Slack messages on failures if(eventData.errorStatus){\tconsole.log(eventData.errorStatus.message); \t// Send message to Slack. \tconst message = createSlackMessage(eventData); \twebhook.send(message); }};// Transform pubsub event message to a stringconst getMessageData = (data) =&gt; { return JSON.parse(Buffer.from(data, 'base64').toString());}// Create the Slack messageconst createSlackMessage = (eventData) =&gt; { let messageParams = eventData.errorStatus.message.split('::'); let message = {\t\"blocks\": [\t\t{\t\t\t\"type\": \"header\",\t\t\t\"text\": {\t\t\t\t\"type\": \"plain_text\",\t\t\t\t\"text\": \":warning: Data Quality Test Failure\",\t\t\t\t\"emoji\": true\t\t\t}\t\t},\t\t{\t\t\t\"type\": \"section\",\t\t\t\"text\": {\t\t\t\t\"type\": \"mrkdwn\",\t\t\t\t\"text\": `The test _${messageParams[0]}_ has failed on table _${messageParams[1]}_. &lt;https://console.cloud.google.com/bigquery?ws=!1m5!1m4!4m3!1s&lt;YOUR PROJECT ID&gt;!2s&lt;DATASET NAME&gt;!3s${messageParams[1]}&amp;project=&lt;YOUR PROJECT ID&gt;|Open table in GBQ&gt;`\t\t\t}\t\t}\t]}; return message;}There’s two things you’ll need to edit in the code: First, put your Slack webhook URL into line 2. Then, update line 43 with your GBQ project id and dataset. That section inserts a link into the Slack message to easily open the offending table in the GBQ web UI, so it needs to know which project and dataset your table is in.That’s it! Go ahead and deploy the code then give it a few minutes for the function to build.Let’s test it outNow that everything is configured, let’s run your first test suite. Go back into the scheduled queries interface and open your scheduled query. Click the “RUN TRANSFER NOW” link at the top of the page and initiate a one time transfer.If everything is good with your data, then absolutely nothing will happen! We don’t want to make too much extra noise, so the cloud function will simply exit if there are no errors.However, if there are any data errors, you should see a new Slack alert pop up about a minute after initiating the transfer run (it takes about a minute in my experience for the transfer runs to complete)." }, { "title": "The Case Against Monolithic Dataflows", "url": "/posts/case-against-monolithic-dataflows/", "categories": "CRM Analytics / Tableau CRM / Einstein Analytics, Dataflows & Recipes", "tags": "Tableau CRM, Einstein Analytics, Dataflow", "date": "2022-04-01 00:00:00 -0500", "snippet": "BackgroundMy project at work this quarter has been tackling various aspects of technical debt in our TCRM environment. Under that banner, one specific initiative that I spearheaded was to break down our main dataflow into smaller pieces. There are many compelling reasons why I wanted to make that change, and I believe that those reasons may also translate to your use case, as well.What’s a monolith?Monolith isn’t a term often used in the Tableau CRM realm, but it’s quite common in software engineering at large. In fact, Professor Wikipedia has the following definition: In software engineering, a monolithic application describes a software application that is designed without modularity. Modularity is desirable, in general, as it supports reuse of parts of the application logic and also facilitates maintenance by allowing repair or replacement of parts of the application without requiring wholesale replacement.It’s with that definition in mind that I define the following characteristics of a monolithic dataflow: It takes 90+ minutes to run It registers a majority of your datasets An error near the end of the dataflow would leave your users with unacceptably stale dataIf you have any dataflows that meet one or more of the criteria above, they’re probably monoliths. But…so what? Why does that matter? This post applies to recipes too, but for simplicity’s sake I’ll just use the term dataflow throughout.The dangers of a monolithDanger #1 - They slow down development of new featuresLet’s say you want to add some additional functionality to an existing dashboard, which requires computing a new field in the dataflow. If that dataflow takes 90+ minutes to run, you’re stuck twiddling your thumbs unable to actually use the new variable for almost 20% of the workday while you wait for your monolithic dataflow to complete.Smaller, more modular dataflows run faster, which ultimately lends itself to a quicker, more nimble, and more iterative development team.Danger #2 - They’re harder for large teams to coordinate changesAt Indeed, we support one of the largest Tableau CRM implementations that I’m aware of. To manage this, we have a team of 6 TCRM developers plus a manager and PM. This means that at times, we have 8 people modifying various dashboards, lenses, recipes and dataflows. And while Salesforce released version control for Tableau CRM, it’s still a far cry from full version control like you find with Git, Subversion, etc because it has no conflict management.So what happens when one of my teammates and I both modify a dataflow at the same time? Whoever saves it last wins. Our team has implemented some excellent control systems to help us manage who “owns” an asset and has priority for modifying it, but accidents still happen. With monolithic dataflows, these accidents are more likely to occur because there are fewer dataflows. By contrast, if you take your single monolithic dataflow and split it into 2, 3, 8, or however many component parts, you’re less likely to encounter conflicts because there are more assets.Obviously, the importance of this factor depends on your team’s size. If you have a small implementation with only 1-2 developers, you can likely communicate easily enough to prevent conflicts. However, as team size increases – and especially if you have a geographically distributed team working from various timezones – the importance of shifting away from a monolithic architecture increases significantly.Danger #3 - They lack scheduling nuance, leading to stale dataSalesforce provides some decent flexibility with regards to scheduling dataflows, allowing you to create a schedule that triggers every X minutes, X hours, etc. But not all data has the same velocity, so when it comes to dataflow scheduling, one size does not fit all.To illustrate, imagine a Salesforce environment where sales reps are creating and working Opportunity records all day. You will probably want to sync the Opportunity object multiple times per day, so that reps always see the latest info. At the same time, say you have a User object that stores records about your sales reps. At many companies, you probably only onboard once per week, so you really only need to refresh the User object once or twice per week.In that scenario, you have data from two objects with two very different velocities. Put together in one dataflow, you’ll encounter one of two results: If you run the dataflow every 4 hours, you’ll be processing the User object too frequently. Outside of the first run after Monday’s onboarding, the dataflow will just process the same records every time. Inefficient. If you run the dataflow once per week, after the User records are updated, your opportunities will be incredibly stale. Sales reps won’t want to use your dashboard because it’s not providing real-time insights.Now obviously that was a somewhat simplified example, but I hope it makes the point. Different data have different velocities and different importance to the business. They need to be broken into separate dataflows based on those factors.Danger #4 - Failed runs lead to even more unacceptably stale dataImagine your data sync runs at 8am daily, followed shortly thereafter by your multi-hour monolithic dataflow. Should that dataflow fail - and I’ve certainly experienced my share of random transient errors - even if you catch the error quickly and re-run the dataflow successfully, you could now be in a situation where it’s 11 o’clock or later and your users are just now getting fresh data for the day.With smaller dataflows which have schedules aligned to their replicated object’s sync schedules (or better yet, are run on demand), you can be assured that your users will get the freshest data available, as soon as it’s available. And when it comes to enabling data-backed business decision making, the importance of fresh data cannot be overstated.The solution: smaller, modular dataflowsNow that I’ve laid out the dangers of a monolith, I hope the solution has become clear. You should probably break your dataflow into smaller chunks. How exactly you decide to split your dataflow should be based on a few factors: How often the records being processed in the dataflow update What time (time of day and day of week) the records being processed update Interdependencies, where dataset X might rely on one or more source objects How critical a particular dataset is to business decision making The size of your team and how often multiple developers need to modify the same dataflow definition How close you are to hitting the 60 flow per 24 hour limit" }, { "title": "Dataflow Extractor", "url": "/posts/dataflow-extractor/", "categories": "Projects", "tags": "", "date": "2022-03-09 00:00:00 -0600", "snippet": "AboutI created this simple web tool to assist in Tableau CRM/Einstein Analytics dataflow development. It allows users to view the datasets being registered within a given dataflow, and then isolate only the nodes required to output a selected dataset(s).The Use CaseIf you’re in an organization like mine, you’ve probably seen some wicked large dataflows. We’re talking hundreds of nodes and multi-hour runtimes. But if you’re working on one particular aspect of the dataflow, say adding a few new variables to a computeExpression node, you’ll have to run the entire dataflow just to test and iterate on the node which you’re adding.Doing things that way can equate to multi-hour turnarounds and a very slow process. This is where the dataflow extractor shines. Upload the JSON file for your dataflow into the extractor, and select the dataset you’re modifying. The extractor will output a new dataflow JSON containing just the nodes needed for your dataset. Now you can work on your computeExpression in this smaller dataflow, which will have a much shorter runtime and allow you to iterate on your changes much more rapidly. When you’re done, simply copy the changes back into your original dataflow.The Manual CaseOf course, you can do all of this manually, too. But manually removing unwanted nodes from a dataflow involves clicking on the little delete icon next to each node you want to remove. Tedious!Check out the dataflow extractorView source code on Github" }, { "title": "SAQL Design Pattern: Reshaping Long to Wide (The Kansas City Shuffle)", "url": "/posts/saql-design-pattern-reshaping-long-wide-kansas-city-shuffle/", "categories": "CRM Analytics / Tableau CRM / Einstein Analytics, SAQL", "tags": "Tableau CRM, Einstein Analytics, SAQL, Business Intelligence", "date": "2021-09-29 00:00:00 -0500", "snippet": "BackgroundWhen building dashboards, I frequently find myself needing to reshape data. Maybe it’s in a long format, but visually would make more sense as a wide table. Maybe it’s the other way around. In fact, I come across this situation so often that I’ve devised design patterns around each scenario. Today I’ll talk about how to transform from long to wide, a transformation that I’ve nicknamed the Kansas City Shuffle.Sample DataLet’s say we have a sample dataset called Variables and it contains two columns: Variable and Value. Here are some sample rows: Variable Value Name Andrew Age 34 Gender Male Location Illinois While that shape can make sense from a data architecture perspective (depending on various factors), it won’t really look good in a dashboard. Instead, we’d probably prefer the data to appear this way: Name Age Gender Location Andrew 34 Male Illinois It’s an easy fix with the Kansas City Shuffle!The Design Pattern-- First we need to load and filter our dataset to get the initital table aboveq = load \"Variables\";q = filter q by &lt;&lt;whatever&gt;&gt;;-- For each row that we want to transform to a column, we'll need a separate variableq1 = filter q by 'Variable' == \"Name\";q1 = foreach q1 generate \"0\" as 'index', 'Value';q2 = filter q by 'Variable' == \"Age\";q2 = foreach q2 generate \"0\" as 'index', 'Value';q3 = filter q by 'Variable' == \"Gender\";q3 = foreach q3 generate \"0\" as 'index', 'Value';q4 = filter q by 'Variable' == \"Location\";q4 = foreach q4 generate \"0\" as 'index', 'Value';/* Now we can cogroup our four queries together to get them in one row. In theory, you should be able to cogroup all four in one call as such: res = cogroup q1 by 'index' left, q2 by 'index', q3 by 'index', q4 by 'index'; However, while this is syntactically correct, in my experience it doesn't work. Instead, we'll need to do 3 cogroups! */res = cogroup q1 by 'index' left, q2 by 'index';res = foreach res generate q1.'index' as 'index', first(q1.'Value') as 'Name', first(q2.'Value') as 'Age';res = cogroup res by 'index' left, q3 by 'index';res = foreach res generate res.'index' as 'index', first(res.'Name') as 'Name', first(res.'Age') as 'Age', first(q3.'Value') as 'Gender';res = cogroup res by 'index' left, q4 by 'index';res = foreach res generate first(res.'Name') as 'Name', first(res.'Age') as 'Age', first(res.'Gender') as 'Gender', first(q4.'Value') as 'Location';-- That's it! Our table is now in a wide format.FAQWhy include ‘index’?We need something to serve as a key when cogrouping the queries together.Why is the index a zero string “0”?Believe it or not, if you generate 0 as ‘index’ and use that to cogroup, SAQL will return an error due to the Numeric type. So I put it in a string. The value of zero is arbitrary.Why use the first() function?Cogroup will automatically group based on the key provided, so we need to use aggregate functions. This can complicate things, especially if our table had included information for more people. If it has 2 people, we can use first() and last() to distinguish. If it has 3+, things get even more complicated.Isn’t there an easier way to do this?Not that I know of. If you know another way, I’d love to hear!" }, { "title": "Marathon Viz", "url": "/posts/marathon-viz/", "categories": "Projects", "tags": "", "date": "2021-07-01 00:00:00 -0500", "snippet": "Project DescriptionFor the final project in my data visualization course as part of the MS Data Science program at DePaul, I had to create complex visualizations from the dataset of my choice. As an avid runner, naturally I wanted to visualize something running related. Unable to find a suitable dataset, I instead created my own using Python to scrape over 2 million marathon finisher results from four of the largest marathons in the world. I then used R and ggplot to create visualizations from this data.Data GatheringTo build the dataset, I used Python to scrape individual finisher results from the websites of the Berlin, New York City, Chicago, and London marathons.Each website was distinct enough that it required its own scraper, however the four websites generally fell into two different categories: plain HTML-based results determined by url parameters, and Javascript-based results powered by a front-end framework such as Angular.Plain HTMLFor the plain HTML pages, once I identified the relevant query string parameters used to generate the list of results, gathering the data was a simple matter of requesting each page of results and then parsing the HTML with BeautifulSoup.However, each of these races had ~ 50,000 finishers per year, and the results websites were limited to between 50 and 1,000 finisher results per page. That translated to a huge number of requests, and a lot of time spent scraping. To speed this up, I was able to use the concurrent.futures library to employ multi-threading and significantly decrease the amount of time required.See the Chicago scraper on GitHub for an example.Javascript-basedFor the JS-based websites, I needed to use Selenium webdriver in order to simulate a user interacting with the browser. This was fairly straightforward, although I did encounter some problems with timeouts. To overcome this, I implemented some simple error-catching that would tell the scraper to go back a page when it reached an unexpected error. Doing so would send a new request to the backend, and generally fix whatever problem had caused the temporary timeout.Here’s a short snippet of the error handling:#loop through each page and gather the resultsnum_error = 0while num_error &lt; 5: try: try_do_scrape(current_page, last_page) except TimeoutException: current_page = int(driver.find_element_by_xpath(\"//li[@class='paginate_button page-item active']\").text) #TimeoutException is always thrown on the final page #If we're on the final page, quit the loop and save if current_page == last_page: break except (UnexpectedAlertPresentException, StaleElementReferenceException) as ex: print(f\"**Encountered exception #{num_error+1}**\") print(ex) time.sleep(1) current_page = int(driver.find_element_by_xpath(\"//li[@class='paginate_button page-item active']\").text) if current_page == last_page: break #Go back a page and start over (we'll dedupe during cleanup) current_page -= 1 prev_button = driver.find_element_by_xpath(\"//a[text()='Previous']\") prev_button.send_keys(Keys.ENTER) driver.execute_script(\"arguments[0].scrollIntoView();\", select_element) num_error += 1 cleanup_and_save(year)Want to see the full code? Check out the Berlin scraper on GitHub.Finishing UpAfter I gathered all the results, I did a small amount of normalizing to match up the column names and saved them all into one big CSV. I also gathered weather-related data (max temp, min temp, and precipitation) from a NOAA website and augmented all of the results with race-day weather for the race location.Data VizNow that I had a dataset, the real work began (this was for a data viz course, after all).EDAOne downsides to my decision to build my own dataset is that my dataset was DIRTY. But this is the real world, right? All data in the real world is dirty. Here are a few EDA visualizations to show the differences in data provided by the race websites.Get To The Real Viz, ManOk, that small caveat about the data quality is out of the way, so let’s move on to the viz. I posed a number of questions to the data, and these are the results I got.Which Race Has the Fastest Finish Times?It looks like Berlin has the fastest finish times! I wonder why that is…Could the weather have anything to do with it?How Does Weather Impact Finish Time?Ok, the weather most definitely impacts finish time! Look at Chicago around 2007, ouch. Both the daily high temp and the daily low look like they affect runners. Berlin looks like it gets just as hot as Chicago, but the lows appear lower. How consistent is that?Which Race Has the Most Predictable Weather?We can see in this graph that New York City has the least variability in both daily highs and daily lows. However, there are only six years of data for New York, so we must be careful in assuming that New York has the best weather. Berlin and Chicago both have identical median high and low temperatures, but Berlin clearly has less variability than Chicago.Pacing StrategiesNext, let’s look at pacing strategies. I performed a calculation based on each runner’s half-marathon split and finish times to classify each runner as either a negative split (faster second half than first half), positive split (slower second half), or even split. For runners to be classified as an even split, I allowed a 10% margin, where each half could be up to 5% slower or 5% faster than the other.I then posed some questions of the pacing strategy data.Which Pacing Strategy is most Common?This is one of the most surprising graphs to me! It’s so common in running circles to hear that negative splits are “best”. But looking at the data, they’re certainly not the most common! I wonder how a runner’s fitness impacts their pacing strategy?How Does Pacing Strategy Vary Based on a Runner’s Overall Speed?Oh, now this is very interesting. The proportion of runners doing negative splits appears to be fairly consistent, regardless of their finish time (which is what the quartiles are based on). However, as runners get slower, the proportion of positive splitters increases. This makes intuitive sense for a few reasons – but let’s see how the weather impacts pacing strategy.How Does Pacing Strategy Differ Based on the Weather?This is somewhat of a unique graph choice, inspired by a mosaic plot. For each year, the graph displays the percent of runners who employed a positive split versus those who employed an even split. As we can see, on cooler years the percent or runners who run even splits increases. However, the scorching Chicago sun takes its toll in hotter years, and significantly more runners end up running a slower second half of the race due to heat.Bonus Viz!Here’s a waffle chart depicting the dominance of Kenyan men at the Berlin marathon.Finally, here’s an animated viz to show when Kipchoge breaks away from the competition during the 2018 Berlin marathon (when he set a new World Record of 2:01:39). To accomplish this graph, I took the top 5 runners from Berlin that year and at each split calculated how far they were behind Kipchoge. Then I pivoted that data so I only had three columns: Name, Split, and TimeBehindLeader. I used those variables with ggplot and gganimate to create the below gif." }, { "title": "My Remote Working Tips", "url": "/posts/remote-working-tips/", "categories": "", "tags": "Remote Work", "date": "2021-04-23 00:00:00 -0500", "snippet": "It would be an understatement to say that the pandemic has changed the American workforce. There are more remote workers than ever and, even when things “go back to normal”, I fully expect that there will be more remote workers than prior to last year. As many companies are discovering, there are myriad benefits to having a remote workforce. Despite these benefits, however, remote work does require a few conscious changes in our mindset and behavior in order to unlock peak benefits. Here are some of the things I do to be an effective remote worker.Take Time To Reflect and PlanBefore the pandemic, going to work meant a commute. For many like myself, this was time spent not only transitioning from home mode to work mode, but it also offered a time to think and reflect about the upcoming workday.Now that commutes only last for the 20 seconds between couch and desk, that time to reflect and plan is in danger of being forgotten.To counter this, I spend the first 5-10 minutes of each day looking at my notes from the day prior. I write down some brief bullets stating what I did yesterday, and copy over any remaining action items that need to be completed today. Not only does this help me plan my day and give me time to mentally shift into work mode, but the notes I make during this short reflection period are exactly what I report in my daily SCRUM meeting.Find The Right Tools To Enhance CommunicationTool #1: Whiteboard ReplacementI have to admit it: for as long as I’ve been working in technology, I’ve been an avid whiteboarder. Nothing works better than a whiteboard for helping to simply complex concepts when communicating with your team.There are many digital whiteboarding tools available for remote workers, and I highly recommend finding one that works for you. For me, I need one with built-in shapes, arrows, etc: I simply cannot draw with a mouse.I use the free, open-source diagrams.net (formerly draw.io). Below is a quick chart I made while trying to debug an issue with some taxonomical records for a USDA project. Not only did the chart help to easily convey the source of the problem (two linked records that should not have been), but it also meant I didn’t have to try pronouncing all of the latin when explaining the problem on a call - I could simply refer to the diagram.Tool #2: Meeting ReplacementNo, I’m not talking about Zoom, Google Meets, or another video chat client. In fact, I think many remote workers are burnt out from constant video calls. While video calls do have their place when trying to discuss particularly important topics, I personally employ the following protocol based on the complexity of the topic I need to discuss:Based on this protocol, when something requires more explanation than can be conveyed textually in Slack/Hangouts, but isn’t quite complex or important enough to warrant a video call, I use a Chrome extension called Chrome Capture - Screenshots &amp; Gifs to make a short GIF depicting the issue. The GIF can be easily sent over chat or email, and even saved to record it for posterity. A GIF can also capture more complicated interactions than a simple screenshot, such as multi-page user flows. Best of all, it saves the team from yet another ad-hoc video call." }, { "title": "The Most Important Aspect of Agile User Stories", "url": "/posts/most-important-aspect-user-stories/", "categories": "", "tags": "Development, Agile", "date": "2021-03-18 00:00:00 -0500", "snippet": "The Most Important Aspect of Agile User StoriesGood communication is essential to any team, and for agile development teams, writing effective user stories is one of the most important means of communication we can engage in. User stories are simple enough in concept, and boil down to something like this: As an X user, I want Y feature, so that I can accomplish Z task.However, too often I find the third element, the why, is muddled or even excluded altogether. Let’s examine three versions of the same user story to see how big of a difference the why aspect makes.Version One: No Why As a content manager, I want to add all countries to a record at once.This story seems straightforward, and is most likely detailed enough that the team’s developer can get started right awya on feature development. However, there’s no why and therefore a misunderstanding is quite possible.For example, the screenshot above provides one possible interpretation of this story. The user can type in country names, they get some nice time-saving autocomplete, and can then save multiple countries with one click of the save button. Story done! Or is it?Version Two: The Redundant WhyThe first story didn’t include a why, so let’s add one: As a content manager, I want to add all countries to a record at once, so that I can associate multiple countries to a record.This story still doesn’t get us where we need to be, because the goal is fairly redundant; in essence, it’s saying “I want to add countries to a record so that I can add countries to a record.” Unfortunately, that misses the fundamental essence underlying the customer’s need.Version Three: Including the Goal As a content manager, I want to add all countries to a record at once, so that I can quickly create records with 180-190 countries.In this version, the key difference – the why – helps to explain how the user will actually be utilizing this new functionality. Not only will they be associating multiple countries to a record, but they’ll be associating hundreds of countries to each record.Now we know why they truly want this feature: it’s not just about the basic functional requirement of associating multiple countries to a record, or even doing it all in one submission or database transcation. Instead, our user will be adding so many countries to each record that being able to add them all in one button click compared to hundreds represents a significant time savings.As a result of this added clarify, we now arrive at a solution that meets the customer’s need. Being able to effectively communicate the purpose behind a user story stops problems before they occur. If a developer had created the feature as depicted in the first image, and then had to go back to re-do it after the business analyst or customer points out that it doesn’t actually meet their needs, we’ve now wasted a significant amount of time. Communication prevents this waste, and keeps our teams operating at maximum velocity and with maximum impact." }, { "title": "Boulder analysis", "url": "/posts/Boulder-Analysis/", "categories": "Projects", "tags": "", "date": "2020-12-09 00:00:00 -0600", "snippet": "Whats on Your Mind, Boulder?Project DescriptionWhile browsing through various open data repositories, I stumbled upon one dataset in particular that intrigued me. It contains three years of emails sent to the city council of Boulder, Colorado. (I downloaded an old dataset with the three years in three separate files. An updated, combined repository is now available) Being a fan of the show Parks &amp; Rec, I was curious to know if the citizens of Boulder were just as crazy as the citizens of Pawnee.The code below is all abridged, but you can see the full source code on GitHub.Let’s see what our data looks like first# Ingest the email dataemails2018 = pd.read_csv('emails-2018.csv', index_col=[6])emails2019 = pd.read_csv('emails-2019.csv', index_col=[6])emails2020 = pd.read_csv('emails-2020.csv', index_col=[6])emails = pd.concat([emails2018, emails2019, emails2020])# Let's see what we're working withprint(emails.info())emails.head()&lt;class 'pandas.core.frame.DataFrame'&gt;Index: 27732 entries, AAMkADQ2ZmVlYWI4LWI1MmEtNDc1NC05ZjhkLTI5YTA3ZDZhYWFkZABGAAAAAABKVzMXYWWETKNC5OzLgZmiBwDAPnDKzs5_QbpGrs-tvK30AAAAAAEMAADAPnDKzs5_QbpGrs-tvK30AAHQ_5gXAAA= to AAMkADQ2ZmVlYWI4LWI1MmEtNDc1NC05ZjhkLTI5YTA3ZDZhYWFkZABGAAAAAABKVzMXYWWETKNC5OzLgZmiBwDAPnDKzs5_QbpGrs-tvK30AAAAAAEMAADAPnDKzs5_QbpGrs-tvK30AALCXMumAAA=Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 SentFrom 27731 non-null object 1 SentTo 27405 non-null object 2 SentCC 5376 non-null object 3 ReceivedDate 27732 non-null object 4 EmailSubject 27562 non-null object 5 PlainTextBody 27652 non-null objectdtypes: object(6)memory usage: 1.5+ MBNone SentFrom SentTo SentCC ReceivedDate EmailSubject PlainTextBody MessageIdentifier ..-abridged-tvK30AAHQ_5gXAAA= Eric Johnson Council NaN 2018-12-31 20:01:59.0000000 +00:00 redevelopment of the Balsam BCH campus Dear council members, After reading Friday's g... ..-abridged-tvK30AAHQ_5gWAAA= Jane McClannan Council NaN 2018-12-31 17:26:45.0000000 +00:00 OAU Unintended Consequences Dear City Council Members, I have recently spe... ..-abridged-tvK30AAHQ_5gVAAA= No Reply Council NaN 2018-12-31 14:09:40.0000000 +00:00 Messages on hold for [6bb18bf32d5a8f2798e95f9c... The following messages, addressed to Council, ... ..-abridged-tvK30AAHQ_5gUAAA= Matt Young Council NaN 2018-12-31 11:47:31.0000000 +00:00 Pls keep shelter open all winter Dear Council -- I urge you to accept the propo... ..-abridged-tvK30AAHQ_5gTAAA= Rabbi Deborah Bronstein Council NaN 2018-12-31 11:24:29.0000000 +00:00 Emergency shelter all winter Dear Members of the City Council, I appreciate... Sentiment analysisAre these emails from enraged citizens, similar to Pawnee? Or do the citizens of Boulder engage in civil discourse with their city officials? Let’s do some sentiment analysis to find out.# Clean the dataemails['ReceivedDate'] = pd.to_datetime(emails['ReceivedDate'])emails['year'] = emails.apply(lambda x: x['ReceivedDate'].year, axis=1)emails['month'] = emails.apply(lambda x: x['ReceivedDate'].month, axis=1)emails['week'] = emails.apply(lambda x: x['ReceivedDate'].weekofyear, axis=1)# Do the sentiment analysissentimentScore = []vader = SentimentIntensityAnalyzer()def GetSentiment(text): score = vader.polarity_scores(str(text)) if (score['compound'] &gt; 0): return 'positive' elif score['compound'] == 0: return 'neutral' else: return 'negative'emails['Sentiment'] = emails.apply(lambda row: GetSentiment(row['PlainTextBody']), axis=1)# Plot the count of sentiment over timeemails = emails.sort_values(by=['year', 'month'])dates = emails.groupby([emails.year, emails.month, emails['Sentiment']]).size()dates.unstack().plot()Who emails the most?Next let’s find out who’s emailing city council the most. I plan to better visualize most of this data in Tableau later, but for now we can use matplotlib to display the top 25 senders.squeaky_wheels = emails.groupby([emails.SentFrom]).size().sort_values(ascending=False).head(25)squeaky_wheels.plot(kind='bar')plt.show()WordcloudNext let’s see what’s on the mind of Boulder residents with a wordcloud.from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator# Get the text into a long stringtext = emails.PlainTextBody.tolist()text = ' '.join(map(str, text))# use darker colors that are easier to readcmap = mpl.cm.Blues(np.linspace(0,1,20))cmap = mpl.colors.ListedColormap(cmap[-10:,:-1])wordcloud = WordCloud(width=550, height=250, max_words=100, colormap=cmap, contour_width=4, background_color='#f0f7fa', stopwords=STOPWORDS, collocations=True, min_word_length=3).generate(text)# increase the plot sizeplt.figure(figsize=(5.5, 2.5), frameon=False)plt.imshow(wordcloud, interpolation='bilinear')plt.axis('off')plt.tight_layout(pad=0)plt.show()plt.savefig('wordcloud.png')" }, { "title": "Baton rouge 311 call dashboard", "url": "/posts/Baton-Rouge-311-Call-Dashboard/", "categories": "Projects", "tags": "", "date": "2020-12-03 00:00:00 -0600", "snippet": "The DataThis dataset consists of 23 columns and 457K rows, and was obtained from Data.gov. It contains a record for every 311 request made by citizens of Baton Rouge, LA between 2016 and November 11, 2020.The GoalFor this viz, I wanted to make a simple, clean KPI dashboard: something that the person in charge of Baton Rouge’s 311 program might look at to get quick insight into the various departments’ performance.The Viz" }, { "title": "Illinois hiring dashboard", "url": "/posts/Illinois-Hiring-Dashboard/", "categories": "Projects", "tags": "", "date": "2020-10-30 00:00:00 -0500", "snippet": "The DataThe dataset for this dashboard is published on Illinois’ Data Portal. It is a CSV file with 15 columns and 154,497 rows at the time of ingest. It lists records of each hiring decision made by the State from 2011 until I downloaded the data on 10/26/2020.The VizOpen on Tableau PublicThe InsightAfter looking at the data available, I created this dashboard in order to answer a few questions. Namely: How often do job openings for Position Title X appear? In which counties are those openings available? What is the salary growth potential for a given career track with the State? How can someone identify current employees in Position X in order to find them on LinkedIn, ask for informational interviews, etc.This dashboard should be able to address all of those questions, and I believe it can serve as a useful tool for people interested in careers with the State.For example, let’s say a savvy young computer programmer wants to get a job with the State. They’ll be looking at the ISA career path; that is, Information Systems Analyst I-III. With the dashboard, they’ll quickly see that the median salary for ISA I is $71K, which increases to $88K for ISA II, and tops out at a median $133K for ISA III. Moreover they’ll see that the vast majority of those positions are only available in Sangamon County (which makes the salary all the more appealing, since it’s not compensating for a Chicago COL).Further analysis will reveal that there have only been 10 hires at the ISA III level, and 9 of those 10 were the same individual. So perhaps this career path is likely to stall out at ISA II. Still, both the level I and II positions have seen steady hiring increases over the years and the enterprising young programmer now also has a list of state employees that they can reach out to on social media in order to start networking. Not a bad research tool, really." }, { "title": "SinoCrypt v1.0 Frequency Analysis", "url": "/posts/sinocrypt-v1-frequency-analysis/", "categories": "", "tags": "Python, Matplotlib, Cryptography, SinoCrypt", "date": "2020-10-26 00:00:00 -0500", "snippet": "Frequency AnalysisSimple substitution ciphers are traditionally susceptible to frequency analysis. In frequency analysis, a cryptanalyst will look at the frequency with which each letter occurs in the ciphertext. In English, certain letters (and groups of letters) occur more often than others. They can then compare the frequency of the ciphertext to the overall frequency in English to start breaking the code. It’s exactly the basis for starting with RSTLNE in Wheel of Fortune.Overcoming FrequencyPart of my goal with SinoCrypt was to allow for one-to-many substitutions. The letter B can be encoded as 了 or 人 or 力 (or any of the 77 Hanzi characters comprised of two strokes), each of which can only be decoded as a B. By giving the algorithm such a large pool of choices for each substitution, defeating frequency analysis should be easy. That said, I initially limited most substitutions to a pool of 3 choices, with some letters getting 10 choices. This was due to A) time constraints, as I found copy and pasting from HanziDB rather tedious, and B) the fact that while the Chinese language has some 77,000 unique characters, not all are in common use today. Creating a ciphertext that is a garbled mess of meaningless characters will stand out, which I’m trying to avoid. Ultimately, I’d like SinoCrypt to be sort of a linguistic steganography, with the algorithm capable of choosing character substitutions that make sense semantically. So long story short, version 1 had fewer substitution choices that were limited to only the most commonly used characters.Assessing FrequencyTo figure out how version 1 stacked up, I created a Python script to first calculate the frequency of each character in a string.def countCharacters(text): blankDict = dict() freqdict = dict() textLength = len(text) # Count the number of unique characters for i in text: if i in blankDict.keys(): blankDict[i] += 1 else: blankDict[i] = 1 # Determine the frequency of each character for k, v in blankDict.items(): freqdict[k] = round((v / textLength) * 100, 2) # Return a sorted list in descending frequency return(dict(sorted(freqdict.items(), key=lambda item: item[1], reverse=True)))I then used Matplotlib to generate some pretty graphs demonstrating the frequencies. The image is below, the full code is on GitHub.ResultsAnalyzing the frequency data shows that v1 was moderately successful in its goal of overcoming frequency analysis. The one-to-many substitution significantly increased the spread of the plot: while the original plaintext only had 18 unique characters, the encoded text had 46 unique characters. Three characters repeated 4 times, two repeated 3 times, ten repeated 2 times, and the rest had only a single occurrence." }, { "title": "Introducing SinoCrypt v1.0", "url": "/posts/introducing-sinocrypt/", "categories": "", "tags": "Python, Cryptography, SinoCrypt", "date": "2020-10-19 00:00:00 -0500", "snippet": "BackgroundI’ve always had a thing for secret codes. Maybe I was inspired by watching the 1998 classic Mercury Rising as a kid. I’m not sure. All I know is, codes are fun. So years ago, while I was working as a language analyst for one of those three-letter agencies, I dreamed up my own code. Recently I took it upon myself to employ my burgeoning Python skills in bringing this code to life. Thus I present SinoCrypt, a deceptively simple single substitution cipher (try saying that three times fast).A Code and Some HintsBefore I reveal how it works, why not try to break it yourself? Here’s an encoded message for you. See if you can determine the original message (which is an English phrase).簿要算题大戳囔整魔是攀察德譬题囊只蹦然馕么翻鬣籍题来翻乙激经说个乙然就馕藩是进说从鬣去嚼器戳题时藤乚新乚灌是下乚道道馕嚼翻是霸新整所乚模魔 Hint #1 Both the plaintext and ciphertext contain 69 characters. Whitespaces, punctuation, numbers, etc are dropped. Hint #2 While I can encode and decode programatically, I could also do both manually. How It WorksThe quickest way to break SinoCrypt is with an understanding of Hanzi, or Chinese characters. While there are over 81,000 unique characters in the Chinese language, each are composed of indivdual pieces called radicals; of which, there are only 214. Each character is then comprised of one or more radicals.Each of these radicals can then be broken down further into strokes (think brush strokes in calligraphy), with each radical having a precise number of strokes which are written in a specific order.Sinocrypt uses this concept of strokes to substitute plaintext English to Chinese, based on the number of strokes. A, being the first letter of the alphabet, is assigned a value of 1 and is rendered as a character with 1 stroke. Plaintext Number of Strokes Ciphertext A 1 一 B 2 十 C 3 子 Increasing RandomnessHowever, a simple one-for-one substitution is vulnerable to frequency analysis. In order to add randomness, and because there are a huge number of characters with 1-26 strokes, I used a dictionary to randomly choose a character with the appropriate number of strokes.Enough Talk, Let’s See Some Codeimport randomcodex = { 'a': [\"一\", \"乙\", \"乚\", \"乚\", \"亅\"], 'b': [\"了\", \"人\", \"力\", \"十\", \"又\", \"二\", \"九\", \"八\", \"七\", \"厂\"], 'c': [\"个\", \"上\", \"大\", \"也\", \"子\", \"下\", \"之\", \"么\", \"小\", \"已\"], 'd': [\"不\", \"中\", \"为\", \"以\", \"方\", \"天\", \"分\", \"心\", \"开\", \"从\"], 'e': [\"他\", \"们\", \"出\", \"可\", \"对\", \"生\", \"发\", \"用\", \"去\", \"只\"], 'f': [\"在\", \"有\", \"地\", \"会\", \"而\", \"那\", \"自\", \"年\", \"过\", \"后\"], 'g': [\"我\", \"这\", \"来\", \"时\", \"你\", \"作\", \"里\", \"没\", \"还\", \"进\"], 'h': [\"的\", \"和\", \"国\", \"到\", \"所\", \"事\", \"经\", \"法\", \"学\", \"现\"], 'i': [\"是\", \"说\", \"要\"], 'j': [\"能\", \"家\", \"都\"], 'k': [\"得\", \"着\", \"理\"], 'l': [\"就\", \"道\", \"然\"], 'm': [\"想\", \"意\", \"新\"], 'n': [\"管\", \"算\", \"需\", \"精\", \"察\", \"境\", \"愿\", \"模\", \"疑\", \"演\"], 'o': [\"题\", \"德\", \"影\"], 'p': [\"整\", \"器\", \"激\"], 'q': [\"藏\", \"戴\", \"翼\"], 'r': [\"翻\", \"覆\", \"鹰\", \"藤\", \"鞭\", \"藩\", \"鳍\", \"蹦\", \"襟\", \"戳\"], 's': [\"警\", \"爆\", \"颤\", \"疆\", \"攀\", \"蹲\", \"藻\", \"簿\", \"瓣\", \"孽\"], 't': [\"魔\", \"籍\", \"耀\", \"灌\", \"嚷\", \"壤\", \"譬\", \"躁\", \"馨\", \"嚼\"], 'u': [\"露\", \"霸\", \"蠢\"], 'v': [\"囊\", \"懿\", \"镶\"], 'w': [\"罐\", \"麟\", \"攫\"], 'x': [\"矗\", \"鑫\", \"衢\"], 'y': [\"囔\", \"馕\", \"鬣\"], 'z': [\"蠼\", \"鱲\", \"鱵\"]}# Function that will replace a single english character with the appropriately coded chinese characterdef encrypt(letter): # Choose a random character replacement from the correct list rand = random.randint(0, len(codex[letter]) - 1) # Return the encrypted character return(codex[letter][rand])To encode, we then loop through the input plaintext = input(\"Enter message to encrypt: \") encryptedMessage = \"\" # Loop through the plaintext string to build the encrypted message for i in plaintext: #drop punctuation, spaces, special characters if i.isalpha(): encryptedMessage += encrypt(i) # Print the encrypted message to the screen print(encryptedMessage)See Source on GitHubRoadmapThis made for a decent first version, but there are improvements that can be made. I need to add more character choices for most of the letters. It was very time consuming to page through Hanzidb’s website to find the most commonly used characters for each stroke count, so I didn’t. I’m working on a webscraper to gather the rest for me. random.randint() is not cryptographically secure, and resulted in a number of repeated characters in the v1 example. Specifically, the portion 乚道道馕 is a chunk that most codebreakers would recognize as ‘ally’ in a heartbeat. Add NLP. As it stands, the generated cipher is complete gibberish linguistically. However, given the number of options for each English letter substitute, I feel like if I scraped enough Mandarin I should be able to generate syntactically correct ciphertext. However, I don’t know really anything about NLP or ML yet, so this might not actually be possible." }, { "title": "Andrew's Analysis: Matt Verlaque of UpLaunch", "url": "/posts/andrews-analysis-matt-verlaque-of-uplaunch/", "categories": "", "tags": "Entrepreneurship", "date": "2020-10-14 00:00:00 -0500", "snippet": "I’m a huge fan of the IndieHackers podcast: the host, Courtland Allen, does an incredible job interview successful tech entrepreneurs. I recently listened to his interview with Matt Verlaque of UpLaunch.com, who went from being a fire fighter with little coding experience to being the CEO of a SaaS company bringing in north of $120,000 MRR. Here are a few of my thoughts.Invest in yourselfThe first takeaway I had from Matt’s interview is the importance of investing in yourself. When bootstrapping a business, money will always be tight, and sacrifices will need to be made. However, Matt and his team used their limited resources to invest back into the business by helping Matt to grow as both a coder and CEO.In both instances, the investment paid dividends. In the case of the coding bootcamp, not only did it allow Matt to create their first custom-built offering, which meant that they no longer had to take the blame for software failures that weren’t their fault, but a classmate he met at the bootcamp would later join UpLaunch as CTO.Time Savings is an Incredibly Effective Value PropositionAnother key takeaway is the effectiveness of time savings as a measure of value for a business-to-business offering. With UpLaunch’s software, they can easily demonstrate to gym owners how much time their product will save – enough to replicate the work of a full-time employee. This makes it exceedingly simple to communicate the value provided: multiply 40 hours per week, 4 weeks per month, by the prevailing wage rate to arrive at the cost of the employee. Subtract the $300 monthly software license, and you have a very precise metric for how much money the product will save.The use of time savings gives UpLaunch a value proposition that is not only easy to quantify and communicate to potential customers, but also (judging by their revenue) exceedingly effective.You Don’t Have to be a Coder to Start a SaaS BusinessFinally, Matt’s story illustrates one more point: you don’t have to be an excellent Bill Gates-level programmer to create a software-based business. Instead, Matt built his product in sensible stages. Matt was able to create his initial product by bundling and configuring already existing, white-label offerings to suit his needs. After using that to validate his idea and build a customer base, he attended a coding bootcamp to create their first custom-built offering.While bootcamps are expensive, typically costing more than $10,000, this was still significantly less than hiring a full-time development team or paying a consulting firm to build the product for them. Matt’s software enabled UpLaunch to scale dramatically, after which he was able to hire a CTO and development team to really take the software to the next level. Matt jokes that the software is now too complex for him to work on anymore, which is perfectly fine: he is now more focused on being the CEO and growing the business. However, this logical progression allowed someone with little programming experience to create and scale a SaaS company in a very effective and efficient manner." }, { "title": "Illinois overdose dashboard", "url": "/posts/Illinois-Overdose-Dashboard/", "categories": "Projects", "tags": "", "date": "2020-10-14 00:00:00 -0500", "snippet": "The DataI gathered the data from a PDF file published by the Illinois Department of Public Health.The VizOpen on Tableau PublicThe InsightOne of the weird things about living in Illinois is that you have Chicago, and then you have everywhere else (“downstate”). They are like distinct entities, each with their own needs, trends, political leanings, etc. Because Chicago has such a large population (roughly 75% of Illinoisans live in the Chicago metro area), it can often be easy to forget about the downstaters - or so those living downstate tend to feel. Anyways, this also means that Chicago’s data often dwarf the rest of the state. Hence when we analyze total overdose deaths in Illinois between 2013 and 2018, we unsurprisingly see that Chicago’s Cook County not only has the most overdoses, but has enough to drown out the rest of the state.However, when we analyze the number of overdoses in a given county per 100K population, we can see that drug overdoses are not just a Chicago problem. In fact, 15 Illinois counties - not one of which rhymes with ‘hook’ - exceed the national average overdose rate. Granted, some of these counties have such a small population that a single overdose will cause it to exceed the 20.7 per 100k figure." }, { "title": "Andrew's Analysis: Laura Roeder Of MeetEdgar And Pre-Launch Strategy", "url": "/posts/andrews-analysis-Laura-Roeder-of-MeetEdgar-and-Pre-Launch-Strategy/", "categories": "", "tags": "Entrepreneurship", "date": "2020-10-01 00:00:00 -0500", "snippet": "This morning I had the pleasure of watching an excellent YouTube video: The Pre-launch Strategy That Built MeetEdgar a 100k List - Laura Roeder at Converted 2016. Here are a few of my take-aways as they pertain to my current entrepreneurial pursuit, Viromob.List Building, Even Prior to Launching, Is Wicked ImportantSure, this point sounds like common sense to many. But, as Laura pointed out, it’s something that can often be missed by software developers turned founder. It can be hard for me to take off my programmer hat and switch into the marketer hat, especially because as a developer I hate showing people incomplete, unpolished products. My instinct is to perfect the software, to iron out every bug and get the UI into pixel-perfect alignment, before I get too many eyes looking at it. I have standards, people.But as a founder - even at the pre-launch wantrapraneur level that I am now - I can’t be trapped in the mentality of a perfectionist web developer. I have to think about marketing, networking, and the million other things that come with running your own business. That means start list building, and start it now.Create ScarcityAnother point I really liked about Laura’s talk was with creating scarcity and how that drove a higher conversion rate for their landing page. Here’s the landing page for MeetEdgar that Laura showed during the talk:Laura went into some detail about how the phrase “Get Your Invitation” converted the best for their team, and even showed some split testing against other variations.I won’t summarize further, as you can always just watch the talk (it’s a very engaging 43 minutes), but I was inspired by both the design of the landing page and the scarcity factor in their call-to-action. I strove to copy emulate it for Viromob’s latest, cleanest landing page, seen below.It’s not perfect, but it’s a start. That, ultimately, is what entrepreneurship is all about: taking a step. Then learning from it and taking another." }, { "title": "Viromob", "url": "/posts/Viromob/", "categories": "Projects", "tags": "", "date": "2020-10-01 00:00:00 -0500", "snippet": "OverviewViromob is a web-based business that I worked on between 2019 and 2020 as a solo founder. Viromob is intended to be a tool for small business owners to promote their business through social-media giveaways. With Viromob, entrepreneurs can easily host a product giveaway where customers earn entries through social actions, such as following the brand on Instagram. Additionally, Viromob has an accompanying platform where users can find new giveaways to enter, see their current giveaways, and more.The Tech StackViromob contains two applications, the Giveaway Entry app and the Sponsor Dashboard app, as well as a Marketing Site.Giveaway Entry App: https://app.viromob.com Progressive Web App Coded with Angular 9 and Ionic Framework Mobile-first designSponsor Dashboard: Angular 9 websiteMarketing Site: https://www.viromob.com Static landing page built with Webflow Content marketing blog powered by Webflow CMS Integrates with Zapier and MailChimpThe BackendOne of the most interesting parts of programming Viromob was working with Google Cloud Firestore and Cloud Functions to create a very powerful headless backend. There are some semi-complex business rules related to a giveaway ending and the winner being drawn, notified, and confirmed that are enforced through Cloud Functions: a blog post detailing this is upcoming. This also tied in nicely with beautiful transactional emails sent through MailJet." }, { "title": "Automatic Image Resizing with Google Cloud Functions", "url": "/posts/automatic-image-resizing-google-cloud-functions/", "categories": "", "tags": "Development", "date": "2020-03-04 00:00:00 -0600", "snippet": "I’m using Firebase as the backend for my current project, Viromob, and recently implemented functionality that will automatically resize any images uploaded to Cloud Storage. There are a few options available online for implementing this, but unfortunately both had some issues that need resolving. For posterity, here’s what I did.Option 1: Use Firebase’s Resize Images ExtensionGoogle makes this incredibly easy, and you can install the extension from either the Firebase console or via the firebase CLI by running:firebase ext:install storage-resize-images --project=projectId_or_aliasThe extension takes a few minutes to install and works rather well, with one small hiccup: the newly created images aren’t publicly visible. Looking at the source code below, you can see that the download token needed to view the resized images is explicitly deleted. While I suppose there could be a security justification for this (explicit deny all then manually requesting access via the makePublic() method does seem more secure), it makes the extension unsuitable for my use case: I want auto-generated thumbnails of uploaded images that will be visible immediately. Here’s the relevant GitHub issue.Option 2: Create your own Cloud FunctionUltimately I ended up making my own function, with many thanks to Jeff Delaney for providing the starting point.const { Storage } = require('@google-cloud/storage');const { tmpdir } = require('os');const { join, dirname } = require('path');const sharp = require('sharp');const fs = require('fs-extra');const UUID = require(\"uuid-v4\");const gcs = new Storage();/** * Resizes all images uploaded to Storage into widths of 160, 320, and 720 * Inspired by https://fireship.io/lessons/image-thumbnail-resizer-cloud-function/*/exports.generateThumbs = functions .runWith({ memory: \"2GB\", timeoutSeconds: \"60\" }) .storage .object() .onFinalize(async object =&gt; { const bucket = gcs.bucket(object.bucket); const filePath = object.name; const fileName = filePath.split('/').pop(); const bucketDir = dirname(filePath); const workingDir = join(tmpdir(), 'thumbs'); const tmpFilePath = join(workingDir, fileName); //Prevent infinite loops by checking if file has already been resized if (fileName.includes('_resize@') || !object.contentType.includes('image')) { console.log('exiting function'); return false; } // 1. Ensure thumbnail dir exists await fs.ensureDir(workingDir); // 2. Download Source File await bucket.file(filePath).download({ destination: tmpFilePath }); // 3. Resize the images and define an array of upload promises const sizes = [160, 320, 720]; const uploadPromises = sizes.map(async size =&gt; { const ext = fileName.split('.').pop(); const oldName = fileName.replace(`.${ext}`, ''); const newName = `${oldName}_resize@${size}.${ext}`; const thumbPath = join(workingDir, newName); // Resize source image await sharp(tmpFilePath) .resize({ width: size }) .toFile(thumbPath); // Upload to GCS return bucket.upload(thumbPath, { destination: join(bucketDir, newName), metadata: { metadata: { firebaseStorageDownloadTokens: UUID() //This makes the image public } } }); }); // 4. Run the upload operations await Promise.all(uploadPromises); // 5. Cleanup remove the tmp/thumbs from the filesystem return fs.remove(workingDir); });This function applies to any image created in your entire storage bucket (even the images created by the function), so it’s important to prevent an infinite loop. I’ll probably update my code to check against a metadata flag for resized images, rather than the name, to prevent edge cases where a user might upload an image that happens to trigger the function exit. Also, since this is bucket-wide, that function exit is a good place to check the current directory if you only want to resize certain images.Additionally, since this function uses Sharp rather than ImageMagick which is used in the Extension, it might end up being much faster. Since I’m just tinkering around on an MVP and don’t have much in the way of users yet, I can’t confirm that though." }, { "title": "Creating Customer Value at Strava", "url": "/posts/creating-customer-value-at-Strava/", "categories": "", "tags": "Case Study, Entrepreneurship", "date": "2019-11-28 00:00:00 -0600", "snippet": "BackgroundStrava is a growth stage startup, which means they have one goal: increasing their customer base. While they have been hugely successful at increasing their user base–to the tune of 1M new users each month–that hasn’t been translating into paying Summit customers. As a long-time and entrepreneurial-minded Strava user (not customer), my take on why is simple: they’re not creating enough value for the customers.How Strava Can Increase ValueStrava has a pretty simple model, which includes a set of free features available to all users and a set of paid features available only to their Summit customers. Increasing the customer base can therefore take one of two approaches: add more Summit features (increase value) or figure out how to better communicate the existing value. While simple, it’s certainly not easy, and figuring out which features to add is where Strava needs strong product management.Value Capture and APIOne area of improvement is in value capture, specifically surrounding Strava’s API. Now, APIs are great. They help to increase the ecosystem around a product, and can even become a full-fledged revenue producing product in and of themselves. Moreover, I believe that Strava’s API represents a strategically significant product moving forward, especially considering the recent trends in the wearable fitness industry (case in point: Google acquiring FitBit).That said, Strava could be utilizing its API to identify and vet potential product enhancements. Just take a look at StravaBestEfforts.com, a site that lets users track their Best Efforts over time.&lt;/p&gt;Best Efforts are purely a creation of Strava: they are a programmatic calculation of how quickly you’ve run a specific distance, based on your GPS data. Strava simply displays your best Best Effort for each distance on your profile, and shows you a little medallion on an activity where you’ve clocked a new top three best effort. But what about this simple user story? “As an athlete, I want to track the improvement in my Best Efforts over time.” Strava doesn’t have anything resembling this functionality, which is no doubt how StravaBestEfforts came into being.So what’s the big deal with this? Simply put, this is value that Strava isn’t capturing from its user data. All that StravaBestEfforts does is retrieve user data from Strava’s API and then sort that data into a table. It doesn’t even have graphs, it’s just an ugly table! (There are some graphs on the Stats page, but they’re pretty bad.) The big thing here is that it also has a paid account, for which it charges $2.99/month. Think about that. It’s providing a very simple data visualization tool (plus a few additional features with Premium, such as weather tracking) for 60% of Strava’s Summit 3-pack price.While I don’t know how many users are paying for this functionality, this is still an incredibly strong demand signal. Even if only a handful of users pay for it, those are users that had to 1) actively search for, seek out, or somehow find this site; 2) provide permission to link their Strava account; 3) shell out their Paypal or credit card information. The conversion rate would skyrocket if all of those steps were conveniently rolled into a Summit pack, let alone if the data were presented in a more attractive fashion.Generalizing API value captureWhile my conclusion is that Strava needs to implement this best effort tracking functionality, the truth is that’s not based on empirical evidence. I’m not a product analyst at Strava, and I don’t have access to their data. I also don’t have insight into the number of paying customers at Strava Best Efforts. So how can we make this decision more empirical, and also generalize it to extend to products other than Strava? Implement tracking analytics on your API if you don’t already have them. Analyze those metrics for a given period of time, sorting by your top API clients, and then go find, use, and understand their product. Figure out what they’re doing with your API data to determine if they’re adding value to your product/ecosystem or if they’re siphoning value. I would argue that StravaBestEfforts is siphoning value that Strava should be capturing itself. Once you’ve identified a value siphoning API client, analyze the API calls to determine the number of users, and try to determine how many of them are paying. These figures should provide a decent basis for determining the potential upside for implementing the feature in your own product. In the case of StravaBestEfforts, free users are limited to 180 days of Best Effort comparison, so it might be possible to differentiate between free and paying users by the activity date ranges included in the API calls.ConclusionThis is getting long, so I’ll wrap up for now. The bottom line is that Strava needs to increase value for its customers, and the most surefire way to do that is by adding more paid features. A smart product leader is one who uses data to drive their decisions, and in this case Strava could be utilizing their API data to vet demand for new features to ensure ROI for their development efforts." }, { "title": "Building Ionic Capacitor Apps for Release", "url": "/posts/building-ionic-capacity-apps-for-release/", "categories": "", "tags": "Development", "date": "2019-11-25 00:00:00 -0600", "snippet": "I’ve been playing around with Ionic for a while now, and recently started using Capacitor while working on a small-scale Android medicine reminder app MedReminder. I haven’t tried Ionic’s AppFlow for managing DevOps yet, but I’ve found that Capacitor certainly makes the app signing process easier since I can just let Google Play manage the signing.However, after publishing to the app store for the first time, I was surprised by the size of my app. It’s a simple Ionic app with only a few pages and very few additional plugins, but was pushing 11 MB. Since then, I was able to reduce the app size by 60%, down to only 4 MB, but I quickly discovered that there is a right and wrong way to do that.The Wrong Way to MinifyWhen you run npx cap open android, you’ll see a build.gradle file that specifies a property called minifyEnabled. Changing this value to true did two things: first, it reduced my app from 11 MB down to 10MB. Second, it broke the app completely and resulted in instant crashing across every test client. Here’s a GitHub issue with the same behavior, which is purportedly due to ProGuard.The Right Way to MinifyThere’s a better approach to minifying that — while not immediately obvious from the Ionic and Capacitor documentation (at least, to me) — is facepalmingly obvious in hindsight. Simply use the following workflow when you’re ready to deploy your production build:ionic build --prodnpx cap copynpx cap open androidThen in Android Studio, do Build &gt; Generate Signed Bundle / APK and select Android App Bundle. Select your upload key, then the ‘release’ build variant and you’re good to go. The final APK size is significantly smaller, which should help to increase downloads and decrease uninstalls.Version 1.0.1: no -prod flag, minifyEnabled=trueVersion 1.1.0: no -prod flag, minifyEnabled=falseVersion 1.2.0: with -prod flag, no minifyEnabled (since -prod does its own minifying)" }, { "title": "Veteran gi bill usage research", "url": "/posts/Veteran-GI-Bill-Usage-Research/", "categories": "Projects", "tags": "", "date": "2017-09-01 00:00:00 -0500", "snippet": "BackgroundWhile enrolled at the University of Illinois, I took an individual studies economics course under the mentorship of Dr. Bryan Buckley. I used the semester to research veterans, and how our collective use of educational benefits has been affected by changes in the GI Bill, as well as various state-level veterans educational benefits.MethodI gathered a large amount of public data from the Department of Veterans Affairs to build a balanced panel with 16 years of usage data across 50 states and Washington DC. I used Stata to perform the analysis and graphing, and ultimately applied a fixed effects model.FindingsThe first significant finding was an update from a 2010 study by Simon, Negrusa, and Warner who predicted that the Post-9/11 GI Bill would increase benefit usage by 20%. My findings indicated an increase of 70%, which is explained, in part, by also including benefit usage by dependents in my analysis.The second (and to me, more interesting) finding was that veterans are absolutely maximizers. The addition of the Post-9/11 GI Bill gave many veterans an option: should they use the new Post-9/11 GI Bill, or should they use the older Montgomery GI Bill? My analysis indicates that in states with their own veteran educational benefits (such as Illinois’ Illinois Veterans Grant or Texas’ Hazlewood Act), veterans are more likely to use the Montgomery GI Bill. This is likely the result of “stacking” both benefits at once to extract the maximum monetary value from both programs.Read the full paper" } ]
